{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsHRnjvfGBvlV2354ausC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayesha-Imr/Gen-AI-Projects/blob/main/RAG_with_LLM_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting up"
      ],
      "metadata": {
        "id": "vON0O1pdI24l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the necessary libraries first."
      ],
      "metadata": {
        "id": "ruwzp5pzJtr6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKB5VL-_WZEs"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain openai weaviate-client langchain_community pypdf tiktoken langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll import userdata in order to get access to the API keys required - openAI API Key and Serper API Key - and set them as environment variables."
      ],
      "metadata": {
        "id": "srDUPlb1J2q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "fXsHB5OqWe6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _set_env(var: str):\n",
        "    if os.environ.get(var):\n",
        "        return\n",
        "    os.environ[var] = userdata.get(var)\n",
        "\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "\n",
        "_set_env(\"SERPER_API_KEY\")"
      ],
      "metadata": {
        "id": "wjGd6rQtWpAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Documents preprocessing"
      ],
      "metadata": {
        "id": "UXQ7AF1IKI1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets load the documents to be used for the RAG tool from google drive. I have made PDFs out of some blogs related to RAG from Data Science Dojo and saved the PDFs in my google drive in a folder.\n",
        "Following are the links to the blogs I have used:\n",
        "\n",
        "https://datasciencedojo.com/blog/rag-with-llamaindex/\n",
        "\n",
        "https://datasciencedojo.com/blog/llm-with-rag-approach/\n",
        "\n",
        "https://datasciencedojo.com/blog/efficient-database-optimization/\n",
        "\n",
        "https://datasciencedojo.com/blog/rag-llm-and-finetuning-a-guide/\n",
        "\n",
        "https://datasciencedojo.com/blog/rag-vs-finetuning-llm-debate/\n",
        "\n",
        "https://datasciencedojo.com/blog/challenges-in-rag-based-llm-applications/"
      ],
      "metadata": {
        "id": "4ZZzcY0vKSLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMIpHhJ3WxWw",
        "outputId": "c1b8dd90-d000-4456-da22-5e77ada8f70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll use Langchain's PyPDFLoader to extract textual content from each pdf and split it into chunks based on pages (each page is a separate chunk)."
      ],
      "metadata": {
        "id": "oB6GxyXYKn7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Path to the folder containing PDFs\n",
        "folder_path = '/content/drive/My Drive/RAG_blogs'\n",
        "\n",
        "# List all PDF files in the folder\n",
        "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
        "\n",
        "# Initialize an empty list to store all pages from all PDFs\n",
        "all_pages = []\n",
        "\n",
        "# Process each PDF file\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(folder_path, pdf_file)\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "\n",
        "    # Load and split pages\n",
        "    pages = loader.load_and_split()\n",
        "\n",
        "    # Append each page to the all_pages list\n",
        "    all_pages.extend(pages)\n",
        "\n",
        "    # Output to confirm each file is processed\n",
        "    print(f\"Processed {pdf_file}, number of pages loaded: {len(pages)}\")\n",
        "\n",
        "# After processing all files, check the total number of pages collected\n",
        "print(f\"Total number of pages collected from all PDFs: {len(all_pages)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jnjQ_DsbqgK",
        "outputId": "d71c5e26-1fc3-409a-e74e-43b1e98bcd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Optimize RAG efficiency with LlamaIndex.pdf, number of pages loaded: 5\n",
            "Processed Retrieval augmented generation.pdf, number of pages loaded: 6\n",
            "Processed Database Optimization.pdf, number of pages loaded: 6\n",
            "Processed RAG and finetuning.pdf, number of pages loaded: 9\n",
            "Processed RAG vs finetuning.pdf, number of pages loaded: 7\n",
            "Processed 12 Challenges in Building Production.pdf, number of pages loaded: 9\n",
            "Total number of pages collected from all PDFs: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the contents of the first element of all_pages."
      ],
      "metadata": {
        "id": "9gseo2AxLMRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_pages[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwfE6P8vdUhP",
        "outputId": "5b67ed37-86ad-4029-e660-e146300d8700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Optimize RAG efficiency with LlamaIndex: The perfect chunk size   \\n \\nMuhammad Jan  \\nOctober 31  \\nRAG integration revolutionized search with LLM, boosting dynamic retrieval.  \\nWithin the implementation of a RAG system, a piv otal factor governing its efficiency and \\nperformance lies in the determination of the optimal chunk size.  How does one identify the most \\neffective chunk size for seamless and efficient retrieval? This is precisely where the comprehensive \\nassessment provide d by the LlamaIndex Response Evaluation tool becomes invaluable.  \\nIn this article, we will provide a comprehensive walkthrough, enabling you to discern the ideal chunk \\nsize through the powerful features of LlamaIndex’s Response Evaluation module.   \\n  \\nWhy chu nk size matters  in RAG system  \\nSelecting the appropriate chunk size is a crucial determination that holds sway over the \\neffectiveness and precision of a RAG system in various ways:   \\n  \\n \\n  \\n  \\nPertinence and detail:  \\nOpting for a smaller chunk size, such as  256, results in more detailed segments. However, this \\nheightened detail brings the potential risk that pivotal infor mation might not be included in the most \\nretrieved segments.  \\nOn the contrary, a chunk size of  512 is likely to encompass all vital information within the leading \\nchunks, ensuring that responses to inquiries are readily accessible. To navigate this challeng e, we \\nwill employ the faithfulness and relevance metrics.  \\nThese metrics gauge the absence of  ‘hallucinations’  and the  ‘relevancy’  of responses concerning the \\nquery and the contexts retrieved, respectively.   \\n  \\n \\nGeneration time for responses:  \\nWith an increase in the chunk size, the volume of information directed into the LLM for generating a \\nresponse also increases. While this can guarantee a more comprehensive context, it might \\npotentially decelerate the system. Ensuring that the added depth d oesn’t compromise the system’s \\nresponsiveness is pivotal.' metadata={'source': '/content/drive/My Drive/RAG_blogs/Optimize RAG efficiency with LlamaIndex.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding and Indexing through Weaviate"
      ],
      "metadata": {
        "id": "VPuedRciLU3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll use weaviate client to embed the PDF text chunks using OpenAI's embeddings and store them in Weaviate vectorstore."
      ],
      "metadata": {
        "id": "FZlwkf30LdXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Weaviate\n",
        "import weaviate\n",
        "from weaviate.embedded import EmbeddedOptions\n",
        "\n",
        "client = weaviate.Client(\n",
        "  embedded_options = EmbeddedOptions()\n",
        ")\n",
        "\n",
        "vectorstore = Weaviate.from_documents(\n",
        "    client = client,\n",
        "    documents = all_pages,\n",
        "    embedding = OpenAIEmbeddings(),\n",
        "    by_text = False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxsUeIRTeN-p",
        "outputId": "9beb901c-22c1-4783-9bb8-539b486d64b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedded weaviate is already listening on port 8079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded weaviate wasn't listening on ports http:8079 & grpc:50060, so starting embedded weaviate again\n",
            "Started /root/.cache/weaviate-embedded: process ID 1013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're going to define our retriever."
      ],
      "metadata": {
        "id": "fjv8UqsqLwRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "1MT5Mjm7f0G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Tools"
      ],
      "metadata": {
        "id": "3SPXBzBeL4YT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we're going to define our tools. First is the retrieval tool for RAG. It will be used to answer user queries related to RAG by fetching relevant information from the vectorstore."
      ],
      "metadata": {
        "id": "EW71gepSL5xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.tools import tool\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain.agents import AgentType, Tool, initialize_agent\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "retrieve_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    name=\"RAG_Blogs_Search\",\n",
        "    description=\"\"\"Fetch information relevant to the user query from a vector store of blogs related RAG (Retrieval Augmented Generation). query: {query}\"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OnG9m_yZgii7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we have a web search tool using Google Serper API which will be used to answer queries unrelated to RAG."
      ],
      "metadata": {
        "id": "Ck6GKxrEwoY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search = GoogleSerperAPIWrapper()\n",
        "\n",
        "search_tool = Tool(\n",
        "        name=\"Web_Search\",\n",
        "        func=search.run,\n",
        "        description=\"Search the web to answer the user query. query: {query}\",\n",
        "    )"
      ],
      "metadata": {
        "id": "wyRg7yPbdaeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding the tools to the the tools list."
      ],
      "metadata": {
        "id": "PIls9W6fMPyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [retrieve_tool, search_tool]"
      ],
      "metadata": {
        "id": "-8Uj45S-duNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Agent"
      ],
      "metadata": {
        "id": "tH74iO6WMTYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create the prompt template for the agent."
      ],
      "metadata": {
        "id": "teT6fg2nMWIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"Use the tools given to you to answer the user query.\n",
        "    If it is a question related to RAG (Retrieval Augmented Generation), then use the retrive_tool to get information about RAG from Data Science Dojo's blogs.\n",
        "    If the query is about something else, use the web_search tool to get the answer, no need to use the retrieve_tool in that case.\n",
        "    Be concise and keep your response limited to 2-3 sentences. Use simple wordings.\n",
        "    Agent Scratchpad: {agent_scratchpad}\n",
        "    Query: {query}\"\"\",\n",
        "    input_variables=[\"query\"])"
      ],
      "metadata": {
        "id": "gbOVT-VKdyV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the LLM, we're using gpt-4 for best results because I found that gpt-3.5 struggles withcalling correct tools and would go back and forth between the two tools needlessly."
      ],
      "metadata": {
        "id": "EMQAnHQzMZ4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)"
      ],
      "metadata": {
        "id": "d99iIYQredNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the agent with our defined LLM, prompt and tools."
      ],
      "metadata": {
        "id": "Lk9xaEDMMkls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n"
      ],
      "metadata": {
        "id": "0UBa_MryeWZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoking the Agent"
      ],
      "metadata": {
        "id": "0QIAGfEgMscJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start by asking a question related to RAG and observe the response trace."
      ],
      "metadata": {
        "id": "imsh_Nv5Mvby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent_executor.invoke({\"query\": \"What are some challenges in RAG?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFEk2BGuefEg",
        "outputId": "1f6c2df2-1ca5-4176-a34a-32bf88a59904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RAG_Blogs_Search` with `{'query': 'challenges in RAG'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mdiscussed potential solutions, highlighti ng various techniques and tools that developers can leverage \n",
            "to optimize RAG system performance and ensure accurate, reliable, and secure responses.  \n",
            "By addressing these challenges, RAG systems can unlock their full potential and become a powerful \n",
            "tool for enhancing the accuracy and effectiveness of LLMs across various applications.\n",
            "\n",
            "12 Challenges in Building Production -Ready RAG based LLM Applications  \n",
            " \n",
            "Fiza Fatima  \n",
            "March 29  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "Large Language Models are growing smarter, transforming how we interact with technology. Yet, \n",
            "they stumble over a significant quality i.e.  accuracy . Often, they provide unreliable information or \n",
            "guess answers to questions they don’t understand —guesses that  can be completely wrong.  Read \n",
            "more  \n",
            "This issue is a major concern for enterprises looking to leverage LLMs. How do we tackle this \n",
            "problem? Retrieval Augmented Generation ( RAG ) offers a viable solution, enabling LLMs to access \n",
            "up-to-date, relevant information, and significantly improving their responses.  \n",
            "Tune in to our podcast and dive deep into RAG, fine -tuning, LlamaIn dex and LangChain in detail!  \n",
            " \n",
            "  \n",
            "Understanding Retrieval Augmented Generation (RAG)  \n",
            "RAG is a framework that retrieves data from external sources and incorporates it into the LLM’s \n",
            "decision -making process. This allows the model to access real -time information and address \n",
            "knowledge gaps. The retrieved data is synthesized with the LLM’s internal training data to generate a \n",
            "response.  \n",
            " \n",
            "Read more:  RAG and finetuning: A comprehensive guide to understanding the two approaches  \n",
            "The challenge of bringing RAG based LLM applications to production  \n",
            "Prototyping a RAG application is easy, but making it performant, robust, and scalable to a large \n",
            "knowledge corp us is hard.  \n",
            "There are three important steps in a RAG framework i.e. Data Ingestion, Retrieval, and Generation. \n",
            "In this blog, we will be dissecting the challenges encountered based on each stage of the \n",
            "RAG   pipeline specifically from the perspective of prod uction, and then propose relevant solutions. \n",
            "Let’s dig in!  \n",
            "Stage 1: Data Ingestion Pipeline  \n",
            "The ingestion stage is a preparation step for building a RAG pipeline, similar to the data cleaning and \n",
            "preprocessing steps in a machine learning pipeline. Usually,  the ingestion stage consists of the \n",
            "following steps:\n",
            "\n",
            "Challenge 3: Creating a Robust and  Scalable  Pipeline:  \n",
            "One of the critical challenges in implementing RAG is creating a robust  and scalable pipeline that can \n",
            "effectively handle a large volume of data and continuously index and store it in a vector database. \n",
            "This challenge is of utmost importance as it directly impacts the system’s ability to accommodate \n",
            "user demands and provide a ccurate, up -to-date information.  \n",
            "1. Proposed Solutions  \n",
            "• Building a modular and distributed system:  \n",
            "To build a scalable pipeline for managing billions of text embeddings, a modular and distributed \n",
            "system is crucial. This system separates the pipeline into scalable units for targeted optimization and \n",
            "employs distributed processing for parallel operation e fficiency. Horizontal scaling allows the system \n",
            "to expand with demand, supported by an optimized data ingestion process and a capable vector \n",
            "database for large -scale data storage and indexing.  \n",
            "This approach ensures scalability and technical robustness in h andling vast amounts of text \n",
            "embeddings.  \n",
            "Stage 2: Retrieval  \n",
            "Retrieval in RAG involves the process of accessing and extracting information from authoritative \n",
            "external knowledge sources, such as databases, documents, and knowledge graphs. If the \n",
            "information is retrieved correctly in the right format, then the answers generated will be correct as \n",
            "well. However, you know the catch. Effective retrieval is a pain, and you can encounter several issues \n",
            "during this important stage.  \n",
            " \n",
            "Common Pain Points in Data Ingestion Pipeline  \n",
            "Challenge 1: Retrieved Data Not in Context  \n",
            "The RAG system can retr ieve data that doesn’t qualify to bring relevant context to generate an \n",
            "accurate response. There can be several reasons for this.  \n",
            "• Missed Top Rank Documents:  The system sometimes doesn’t include essential documents \n",
            "that contain the answer in the top results  returned by the system’s retrieval component.  \n",
            "• Incorrect Specificity:  Responses may not provide precise information or adequately address \n",
            "the specific context of the user’s query  \n",
            "• Losing Relevant Context During Reranking:  This occurs when documents containi ng the \n",
            "answer are retrieved from the database but fail to make it into the context for generating an \n",
            "answer.  \n",
            "Proposed Solutions:  \n",
            "• Query Augmentation:  Query augmentation enables RAG to retrieve information that is in \n",
            "context by enhancing the user queries wit h additional contextual details or modifying them \n",
            "to maximize relevancy. This involves improving the phrasing, adding company -specific \n",
            "context, and generating sub -questions that help contextualize and generate accurate \n",
            "responses\n",
            "\n",
            "Hence, this provides a comprehensive introduction to RAG and fine -tuning, highlighting their roles in \n",
            "advancing the capabilities of large language models (LLMs). Some key points to take away fro m this \n",
            "discussion can be put down as:  \n",
            "• LLMs struggle with providing up -to-date information and excelling in specialized domains.  \n",
            "• RAG addresses these limitations by incorporating external information retrieval during \n",
            "response generation, ensuring informative  and relevant answers.  \n",
            "• Fine -tuning refines pre -trained LLMs for specific tasks, enhancing their expertise and \n",
            "performance in those areas.\u001b[0m\u001b[32;1m\u001b[1;3mSome challenges in Retrieval Augmented Generation (RAG) include creating a robust and scalable pipeline that can handle large volumes of data and continuously index and store it in a vector database. Another challenge is the retrieval of data that is not in context, which can be due to missed top rank documents, incorrect specificity, or losing relevant context during reranking. Solutions to these challenges include building a modular and distributed system for scalability and using query augmentation to retrieve information that is in context.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets see what happens if we ask a question unrelated to RAG."
      ],
      "metadata": {
        "id": "Z3HSUGiXM2UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent_executor.invoke({\"query\": \"What is Data Science Dojo?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFFOmr1dgKbR",
        "outputId": "3f93f4de-417f-4500-88b1-e7158aa08542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Web_Search` with `What is Data Science Dojo?`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mData Science Dojo: Educational institution in Bellevue, Washington. Data Science Dojo Address: 2331 130th Ave NE, Bellevue, WA 98005. Data Science Dojo Hours: Closed ⋅ Opens 8 AM. Data Science Dojo Phone: (877) 360-3442. \"Extremely good bootcamp whereby you get multiple flavors of data science from theory to the practical applications. An intensive one-week exercise with ... Data Science Dojo is an e-learning company that is redefining the data science, large language models, and generative AI education landscape with a simpler, ... Data Science Dojo believes that anyone can learn data science, and provides comprehensive, hands-on training that helps students jump into practical data ... Data Science Dojo is redefining the education landscape of data science, large language models, and generative AI education landscape with a ... Text data, with its high complexity, posits an exciting challenge for the causal inference community. ... practical solution that can bring real business value. Data Science Dojo was founded in 2013 but it was a free Meetup group long before the official launch. With the aim to bring the knowledge of data science to ...\u001b[0m\u001b[32;1m\u001b[1;3mData Science Dojo is an educational institution based in Bellevue, Washington. It is an e-learning company that is redefining the data science, large language models, and generative AI education landscape. They believe that anyone can learn data science and provide comprehensive, hands-on training to help students jump into practical data science.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3aY-41qJiTXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}